{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello Kushal, my name is **Lyuman** and I'm going to review your project!\n",
    "\n",
    "You can find my comments in <font color='green'>green</font>, <font color='blue'>blue</font> or <font color='red'>red</font> boxes like this:\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Success:</b> if everything is done succesfully\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Improve: </b> \"Improve\" comments mean that there are tiny corrections that could help you to make your project better.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>Needs fixing:</b> if the block requires some corrections. Work can't be accepted with the red comments.\n",
    "</div>\n",
    "\n",
    "### <font color='orange'>General feedback</font>\n",
    "* I'm glad to say that you've done a really good job,\n",
    "* I liked that you used GridSearchCV correctly.\n",
    "* It was very pleasant and interesting to read your thoughts and conclusions very much.\n",
    "* I'm accepting this work and wish you best of luck in your future studies!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project description\n",
    "Mobile carrier Megaline has found out that many of their subscribers use legacy plans. They want to develop a model that would analyze subscribers' behavior and recommend one of Megaline's newer plans: Smart or Ultra.\n",
    "You have access to behavior data about subscribers who have already switched to the new plans (from the project for the Statistical Data Analysis course). For this classification task, you need to develop a model that will pick the right plan. Since you’ve already performed the data preprocessing step, you can move straight to creating the model.\n",
    "Develop a model with the highest possible accuracy. In this project, the threshold for accuracy is 0.75. Check the accuracy using the test dataset.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data description\n",
    "Every observation in the dataset contains monthly behavior information about one user.\n",
    "\n",
    "The information given is as follows:\n",
    "\n",
    "\n",
    "сalls — number of calls,\n",
    "\n",
    "\n",
    "minutes — total call duration in minutes,\n",
    "\n",
    "\n",
    "messages — number of text messages,\n",
    "\n",
    "\n",
    "mb_used — Internet traffic used in MB,\n",
    "\n",
    "\n",
    "is_ultra — plan for the current month (Ultra - 1, Smart - 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let us load all relevant packages and import the data \n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import precision_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Success:</b> Great that all imports were collected in the first cell!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://code.s3.yandex.net/datasets/users_behavior.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3214 entries, 0 to 3213\n",
      "Data columns (total 5 columns):\n",
      "calls       3214 non-null float64\n",
      "minutes     3214 non-null float64\n",
      "messages    3214 non-null float64\n",
      "mb_used     3214 non-null float64\n",
      "is_ultra    3214 non-null int64\n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 125.7 KB\n"
     ]
    }
   ],
   "source": [
    "#Let's see if we have any NA\n",
    "df.isna().sum()\n",
    "df.info()\n",
    "#Let's study the data types of the dataframe\n",
    "df['messages'] = df['messages'].astype(int) \n",
    "df['calls'] = df['calls'].astype(int) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Success:</b> Data loading and initial analysis are well done.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are no NA values, so we didn't have to do any extra processing work. Calls and messages don't need to be float so I converted them to int. \n",
    "\n",
    "# Dataset splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now, we will split the data into training, testing and validation sets. Of the base dataset,\n",
    "#I will split 20% for testing and 80% for training.\n",
    "features = df.drop(columns=['is_ultra'])\n",
    "target =  df['is_ultra']\n",
    "features_train, features_test, target_train, target_test = train_test_split(features, target, test_size=0.20, random_state=12345)\n",
    "features_train, features_valid, target_train, target_valid = train_test_split(features_train, target_train, test_size=0.2, random_state=12345 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2056\n",
      "2056\n",
      "643\n",
      "643\n",
      "515\n",
      "515\n"
     ]
    }
   ],
   "source": [
    "print(len(features_train))\n",
    "print(len(target_train))\n",
    "print(len(features_test))\n",
    "print(len(target_test))\n",
    "print(len(features_valid))\n",
    "print(len(target_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check to see if the function worked correctly, I manually checked if the sizes of the training, testing and validation dataset numbers were correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Success:</b> Data splitting was done well. Great that you print out information about sets after splitting!\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter tuning \n",
    "\n",
    "I will investigate the quality of different models by changing hyperparameters, and then briefly describe the findings of my study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.745136186770428"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#First, let fit the datasets to a Logistic Regression model\n",
    "LogRegMod = LogisticRegression(random_state=12345, solver='liblinear') \n",
    "LogRegMod.fit(features_train, target_train) \n",
    "LogRegMod.score(features_train, target_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that logistic regression has an accuracy of 74.5% which is not too bad but we can definitely do better with other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.828307392996109"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Now, let us fit a decision tree model\n",
    "DecTreeMod = DecisionTreeClassifier(random_state=12345, max_depth=5)\n",
    "\n",
    "DecTreeMod.fit(features_train, target_train)\n",
    "DecTreeMod.score(features_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a random guess, I decided to see how a model with max depth 5 would do. We already have a better accuracy than the logistic regression. Let's see if there are better hyperparameters we can find by doing an exhaustive search using the sklearn function 'GridSearchCV'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=3,\n",
      "                       max_features=None, max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, presort=False,\n",
      "                       random_state=12345, splitter='best')\n"
     ]
    }
   ],
   "source": [
    "depth_param = {'max_depth':range(1,25)}\n",
    "DecTreeMod = DecisionTreeClassifier(random_state=12345)\n",
    "DecTreeModOpt = GridSearchCV(DecTreeMod,depth_param)\n",
    "DecTreeModOpt.fit(features_train, target_train)\n",
    "DecTreeModOpt.score(features_train, target_train)\n",
    "print(DecTreeModOpt.best_estimator_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going as deep as 25 layers (arbitrarily chosen), we see that the optimal accuracy is obtained at a depth of 3. Since we know that a tree with a depth of 3 is good enough, let's further narrow the parameter space that the GridSearchCV function needs to look for the Random Forest model with max depth of 10. Having more than 50 estimators also doesn't feel parsimonious so based on pure feelings, I will set that as the range for the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "                       max_depth=8, max_features='auto', max_leaf_nodes=None,\n",
      "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "                       min_samples_leaf=1, min_samples_split=2,\n",
      "                       min_weight_fraction_leaf=0.0, n_estimators=46,\n",
      "                       n_jobs=None, oob_score=False, random_state=12345,\n",
      "                       verbose=0, warm_start=False)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8740272373540856"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depth_param = {'max_depth':range(1,10), 'n_estimators':range(1,50)}\n",
    "RandForestMod = RandomForestClassifier(random_state=12345)\n",
    "RandForestOpt = GridSearchCV(RandForestMod,depth_param)\n",
    "RandForestOpt.fit(features_train, target_train)\n",
    "print(RandForestOpt.best_estimator_)\n",
    "RandForestOpt.score(features_train, target_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took a very long time to run. If run time is a priority, the parameter space needs to be greatly reduced. Here, we see that max depth of 8 worked best, along with 46 estimators. 46 estimators seems like too much so the company needs to evaluate whether they need this marginal increase in accuracy at the cost of having a bulky model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Success:</b> The tuning hyperparameters was done great! GreatSearch was correctly used.  \n",
    "however, I don’t understand why you didn’t use validation set if you declared it earlier.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check the quality of the model using the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80248833592535"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test_accuracy = features_test\n",
    "predictions_test_accuracy = RandForestOpt.predict(features_test_accuracy)\n",
    "quality = accuracy_score(target_test, predictions_test_accuracy)\n",
    "quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, the threshold for accuracy is 0.75. We see that the Random Forest model has an accuracy of 80.2%, which is good enough for us. We don't need to check the other models since this one had a better accuracy on the training data, but out of curiosity, let's see how the decision tree model, our second best one, does on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7869362363919129"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_test_accuracy = features_test\n",
    "predictions_test_accuracy = DecTreeModOpt.predict(features_test_accuracy)\n",
    "quality = accuracy_score(target_test, predictions_test_accuracy)\n",
    "quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even the decision tree model has a good enough result, but as we predicted, the Random Forest Classifier outperforms it. Now, let's look into the precision scores out of curiosity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5051020408163265"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = precision_score(RandForestOpt.predict(features_test_accuracy), target_test)\n",
    "precision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4489795918367347"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = precision_score(DecTreeModOpt.predict(features_test_accuracy), target_test)\n",
    "precision "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of our models, unfortunately, have a very low precision. This means that the chance that we will pick the Ultra when the plan is actually Smart and vice versa (false positive) is quite high. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The data was so big and exhibited a pattern apparent enough that all 3 classification algorithms I implemented performed decently. Precision is a concern, but since the requirement was to focus on accuracy, we don't need to explore that for now. Random forest and decision tree algorithms had a slight difference in performance in terms of accuracy, but tuning the hyperparameters for the random forest classifier is a very computationally intensive task.\n",
    "\n",
    "Finally, despite our very liberal hyperparameter requirements, we managed to avoid overfitting. We know this because our models performed very well on the test dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Success</b> Testing was processed in a good way!\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
